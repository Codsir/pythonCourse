{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599386885197",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### action\n",
    "文本抄袭自动检测分析:\n",
    "如果你是某新闻单位工作人员（这里假设source=新华社），为了防止其他媒体抄袭你的文章，你打算做一个抄袭自动检测分析的工具\n",
    "1）定义可能抄袭的文章来源\n",
    "2）与原文对比定位抄袭的地方\n",
    "原始数据：sqlResult.csv，共计89611篇\n",
    "从数据库导出的文章，字段包括：id, author, source, content, feature, title, url\n",
    "常用中文停用词：chinese_stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(89611, 7)\n文章内容为空的数量: 2557\n删除空内容的文章后数量: 87054\n"
    }
   ],
   "source": [
    "#数据加载与预览\n",
    "\n",
    "artical = pd.read_csv('E:/bi_course/biCourse/L3/tue_course/sqlResult.csv', encoding = 'gb18030')\n",
    "print(artical.shape)\n",
    "\n",
    "#文章内容为空的数量\n",
    "print(\"文章内容为空的数量:\", len(artical[artical.content.isna()]) )\n",
    "\n",
    "#删除文章内容为空\n",
    "artical.dropna(subset = ['content'], inplace = True)\n",
    "print(\"删除空内容的文章后数量:\", artical.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#对文章进行分词处理\n",
    "\n",
    "f = open(\"E:/bi_course/biCourse/L3/tue_course/chinese_stopwords.txt\",\"r\", encoding = 'utf-8')\n",
    "stopwords = [ word[:-1] for  word in f.readlines()] #[:-1]表示删除\\n字符\n",
    "\n",
    "def split_text(text):\n",
    "    text = text.replace(' ','').replace('\\r','').replace('\\n','') #\\r 回车, \\n换行\n",
    "    words = jieba.cut(text) \n",
    "    result  = ' '.join([w for  w in words if w not in stopwords] )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache D:\\UserData\\Zoni\\Temp\\jieba.cache\nLoading model cost 1.003 seconds.\nPrefix dict has been built succesfully.\n"
    }
   ],
   "source": [
    "corpus = list( artical.content.apply( lambda x : split_text(x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#计算语料库corpus的TD-IDF\n",
    "\n",
    "cv = CountVectorizer(encoding = 'gb18030', min_df= 0.015)\n",
    "tfiftransformer = TfidfTransformer()\n",
    "contervector = cv.fit_transform(corpus)\n",
    "# print(contervector)\n",
    "tfdif = tfiftransformer.fit_transform(contervector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标记是否为新华社的新闻\n",
    "label = list(artical.source.apply(lambda x: 1 if x == '新华社'  else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "准确率: 0.83\n精确率: 0.9649\n召回率: 0.8423\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfdif.toarray(), label, train_size = 0.7)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('准确率:', round(accuracy_score(y_test, y_predict),4))\n",
    "print('精确率:', round(precision_score(y_test, y_predict),4))\n",
    "print('召回率:', round(recall_score(y_test, y_predict),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-80ddf55144bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#使用模型进行风格预测\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfdif\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "#使用模型进行风格预测\n",
    "\n",
    "prediction = clf.predict(tfdif.toarray())\n",
    "actual = np.array(label)\n",
    "\n",
    "#比较预测风格和是否为新华社文章:\n",
    "comparison = pd.DataFrame({'articalId':artical.id,'prediction':prediction, 'actual':actual})\n",
    "comparison['copy'] = 0\n",
    "comparison.loc[(comparison['prediction'] == 1 ) & (comparison['actual'] == 0), 'copy'] = 1 #预测文章是新华社 实际不是新华社, 则有抄袭嫌疑\n",
    "copy_article = comparison[comparison['copy'] == 1]\n",
    "xinhuashe_article = comparison[comparison['actual'] == 1]\n",
    "\n",
    "print('疑是抄袭文章数量:', len(copy_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(87054,)\n"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "normalizer = Normalizer()\n",
    "scaled_array = normalizer.fit_transform(tfdif.toarray())\n",
    "\n",
    "#使用聚类对全部文章进行聚类: 分为30类\n",
    "\n",
    "km = KMeans( n_clusters= 30)\n",
    "k_labels = km.fit_predict(scaled_array)\n",
    "print(k_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'copy_article' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a1759384f973>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcopy_article\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'copy_article' is not defined"
     ]
    }
   ],
   "source": [
    "copy_article.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建id_class\n",
    "\n",
    "id_class = {index: class_ for index, class_ in enumerate (k_labels)} #文章经过kmeans的分类\n",
    "\n",
    "from collections import defaultdict\n",
    "class_id = defaultdict(set)\n",
    "\n",
    "for index, class_ in id_class.items():\n",
    "    if index in xinhuashe_article.index.to_list():\n",
    "        class_id[class_].add(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#查找相似文章\n",
    "\n",
    "def search_similar_article ( cpindex, top = 10):\n",
    "    #只在新华社发布的文章中找\n",
    "    dist_dict = {i: cosine_similarity(tfdif[cpindex], tfidf[i]) for i in class_id [id_class[cpindex]]}\n",
    "    return sorted (dist_dict.items(), key = lambda x:x[1][0], reverse= True)[:top]\n",
    "\n",
    "\n",
    "\n",
    "cpindex = \n",
    "\n",
    "similar_list = search_similar_article(cpindex)\n",
    "print(similar_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('怀疑抄袭:\\n', artical.iloc[cpindex].content)\n",
    "\n",
    "#找一篇相似的原文\n",
    "similar2 = similar_list[0][0]\n",
    "print('相似的原文:\\n', news.iloc[similar2].content)"
   ]
  }
 ]
}